# Autoencodeur Variationnel pour la G√©n√©ration de Donn√©es Tabulaires

## Introduction aux Autoencodeurs Variationnels (VAE)

Les Autoencodeurs Variationnels (VAE) sont une extension des autoencodeurs classiques, introduits pour apprendre une distribution latente des donn√©es. Contrairement aux autoencodeurs classiques, les VAE ne se contentent pas d'apprendre une repr√©sentation compress√©e des donn√©es, mais contraignent cette repr√©sentation √† suivre une distribution probabiliste, souvent une distribution normale multivari√©e.

### Principe des VAE

Les VAE sont compos√©s de deux parties :

1. **L'encodeur** : Il prend une entr√©e \( x \) et l'encode en une distribution latente param√©tr√©e par une moyenne `Œº` et une variance `œÉ¬≤`, formant un espace latent \( z \) :

    ```math
   q_\phi(z|x) = \mathcal{N}(\mu(x), \sigma^2(x))
   ```


2. **Le d√©codeur** : Il prend un √©chantillon \( z \) de cette distribution latente et tente de reconstruire les donn√©es d'origine :
   ```math
   p_\theta(x|z)
   ```
La perte du VAE est compos√©e de deux termes :
- **L'erreur de reconstruction** : Mesure la qualit√© de la reconstruction des donn√©es d'origine √† partir de l'espace latent.
- **La divergence de Kullback-Leibler (KL)** : Encourage l'espace latent √† suivre une distribution normale multivari√©e `ùí©(0, I)`, facilitant la g√©n√©ration de nouvelles donn√©es.

L'objectif global du mod√®le est de minimiser :  
```math
\mathcal{L} = \mathbb{E}_{q_\phi(z|x)} [ \log p_\theta(x|z) ] - D_{KL} (q_\phi(z|x) || p(z))
```

## Architecture du VAE pour Donn√©es Tabulaires

### 1. Encodeur
L'encodeur prend en entr√©e un vecteur de caract√©ristiques tabulaires et l'encode dans un espace latent de dimension r√©duite.
- Utilisation de couches fully connected (Dense)
- Activation de type ReLU
- Deux sorties : une pour la moyenne (`Œº`) et une pour la variance (`œÉ¬≤`)

### 2. √âchantillonnage
L'√©chantillonnage de `z` est effectu√© via la **reparam√©trisation** :
```math
z = \mu + \sigma \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
```
### 3. D√©codeur
Le d√©codeur reconstruit les donn√©es d'origine √† partir de `z` via plusieurs couches fully connected avec une activation appropri√©e selon le type de variable (sigmo√Øde pour les variables binaires, softmax pour les cat√©gories, lin√©aire pour les r√©els).

### 4. Fonction de perte
La fonction de perte combine :
- **Erreur de reconstruction** : Mesur√©e avec l'**erreur quadratique moyenne (MSE)** pour les valeurs continues et l'**entropie crois√©e** pour les variables cat√©goriques.
- **Divergence KL** : Encourage `q_œÜ(z|x)` √† suivre `p(z)`.

## G√©n√©ration de Donn√©es Synth√©tiques
Une fois entra√Æn√©, on g√©n√®re des donn√©es en :
1. √âchantillonnant un `z` depuis une distribution normale `ùí©(0, I)`
2. Passant `z` dans le d√©codeur pour obtenir de nouvelles donn√©es synth√©tiques

## Conclusion
Les VAE sont puissants pour g√©n√©rer des donn√©es tabulaires r√©alistes tout en pr√©servant leurs distributions statistiques. Ils sont utilis√©s dans diverses applications comme l'anonymisation de donn√©es, l'augmentation de jeux de donn√©es et la mod√©lisation g√©n√©rative dans le domaine m√©dical et financier.
